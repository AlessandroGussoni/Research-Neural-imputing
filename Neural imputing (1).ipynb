{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing imputing strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook i will implement and evaluate different strategies to adress missing values. As last attempt i will use and autoencoder to reconstruct flawed instances within the dataset. I decided to use a small datataset fro UCI about glass classification, you can find it here: https://www.kaggle.com/uciml/glass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.rc_context at 0x1f668c8de80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "# common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# setting random seed\n",
    "np.random.seed(4)\n",
    "tf.random.set_seed(4)\n",
    "\n",
    "# Style setup\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xkcd(False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Aless\\Downloads\\\\glass.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no missing values in the dataset i am going to artificially create them. For the purpose of the project i will add one missing value to the 10% of the rows. Note that since this is a very small dataset the classifier will be more affected by the changing in the training set, so we hope to see significant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_size = 0.1\n",
    "random = np.arange(df.shape[0])\n",
    "np.random.shuffle(random)\n",
    "missing_rows = random[:int(missing_size * df.shape[0])]\n",
    "data = df.drop(missing_rows, axis = 0).values \n",
    "missing_data = df.loc[missing_rows].values\n",
    "missing_cols = np.random.randint(0, data.shape[1] - 1, int(missing_size * data.shape[0]))\n",
    "zips = list(zip(range(missing_data.shape[0]), missing_cols))\n",
    "for zip_ in zips:\n",
    "    missing_data[zip_[0], zip_[1]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First strategy: dropping missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As null accuracy i am going to evaluate a classifier on a smaller set without missing values. As base classifier i choose a support vector machine, since it is known that achieve good results even though the reduced number of instances. But first let us preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "st_sc = StandardScaler()\n",
    "st_sc.fit(X_train)\n",
    "X_train = st_sc.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on cross validation:  0.5794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "svc = SVC(gamma = 'scale', class_weight = 'balanced')\n",
    "auc = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(svc, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "print('Mean accuracy on cross validation: ', np.round(scores.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second strategy i am going to use various types of imputer: this strategy basically consists in replacing missing values according to a premade strategy. This approach will actually increase the number of intances but it will introduce a certain degree of approximation in our process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on cross validation with strategy = median:  0.52862\n",
      "Mean accuracy on cross validation with strategy = mean:  0.53507\n",
      "Mean accuracy on cross validation with IterativeImputer:  0.55305\n"
     ]
    }
   ],
   "source": [
    "df.iloc[missing_rows, missing_cols] = np.nan\n",
    "def compute_imputer_scores(strategy, iterative = False):\n",
    "    if not iterative:\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        # transforming the data with the imputer\n",
    "        imputer = SimpleImputer(missing_values = np.nan, strategy = strategy)\n",
    "        data = imputer.fit_transform(df.values)\n",
    "        # creating the set\n",
    "        X = data[:, :-1]\n",
    "        y = data[:, -1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "        # preprocessing\n",
    "        st_sc = StandardScaler()\n",
    "        st_sc.fit(X_train)\n",
    "        X_train = st_sc.transform(X_train)\n",
    "        # cross validating\n",
    "        svc = SVC(gamma = 'scale', class_weight = 'balanced')\n",
    "        auc = make_scorer(roc_auc_score)\n",
    "        scores = cross_val_score(svc, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "        return scores\n",
    "    \n",
    "    else:\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "        # transforming the data with the imputer\n",
    "        imputer = IterativeImputer(missing_values = np.nan, max_iter = 50)\n",
    "        data = imputer.fit_transform(df.values)\n",
    "        # creating the set\n",
    "        X = data[:, :-1]\n",
    "        y = data[:, -1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "        # preprocessing\n",
    "        st_sc = StandardScaler()\n",
    "        st_sc.fit(X_train)\n",
    "        X_train = st_sc.transform(X_train)\n",
    "        # cross validating\n",
    "        svc = SVC(gamma = 'scale', class_weight = 'balanced')\n",
    "        auc = make_scorer(roc_auc_score)\n",
    "        scores = cross_val_score(svc, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "        return scores\n",
    "    \n",
    "print('Mean accuracy on cross validation with strategy = median: ', np.round(compute_imputer_scores(strategy = 'median').mean(), 5))\n",
    "print('Mean accuracy on cross validation with strategy = mean: ', np.round(compute_imputer_scores(strategy = 'mean').mean(), 5))\n",
    "print('Mean accuracy on cross validation with IterativeImputer: ', np.round(compute_imputer_scores(strategy = None, iterative = True).mean(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cross validate accuracy seemed to be affected in a negative way from the imputing strategies. Note that this is not a general rule, but it can happens that a bad imputation lead to worse results expecially on small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural imputing with an autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is basically a neural network which try to reproduce its inputs, forcing the data to pass through a bottle neck. This will force the network to learn pattern inside the data in order to map them to the so called latent space (the bottle neck previous mentioned). We can use this poerful idea to build a small autoencoder able to reconstruct instances with a missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 604.1664\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 570.8040\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 559.5662\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 558.4683\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 557.4935\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 556.6599\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 555.8860\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 555.1683\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 554.4670\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 553.8046\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 553.1494\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 552.5139\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 551.8355\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 551.0177\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 974us/step - loss: 549.9653\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 548.4466\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 545.1716\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 535.3654\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 508.7173\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 455.8221\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 383.4904\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 304.0762\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 213.6963\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 122.5452\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 53.9134\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 17.7040\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 4.8204\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.5316\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0715\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0512\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0519\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0536\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0521\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0540\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0514\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0541\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0511\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0491\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0444\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0492\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0574\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0495\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0462\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0458\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0482\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0493\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0462\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0479\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0447\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0486\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0784\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0471\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0645\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0899\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0651\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0685\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0681\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0568\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0521\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0438\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0456\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0495\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0653\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0448\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0421\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 998us/step - loss: 1.0787\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0404\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 922us/step - loss: 1.0414\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 916us/step - loss: 1.0486\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0459\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0429\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 933us/step - loss: 1.0620\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0440\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0456\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0491\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 992us/step - loss: 1.0432\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 936us/step - loss: 1.0523\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0420\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 988us/step - loss: 1.0401\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0396\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 942us/step - loss: 1.0540\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0381\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0429\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0468\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0411\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0424\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0402\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0386\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 997us/step - loss: 1.0368\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0406\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 990us/step - loss: 1.0412\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 996us/step - loss: 1.0361\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 921us/step - loss: 1.0390\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.089 - 0s 1ms/step - loss: 1.0407\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0361\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0407\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0631\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0400\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0391\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.0604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f605140cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codings_size = 2\n",
    "\n",
    "encoder = keras.models.Sequential([keras.layers.Flatten(input_shape = [data.shape[1]]),\n",
    "                                   keras.layers.Dense(8, activation = 'relu'),\n",
    "                                   keras.layers.Dense(5, activation = 'relu'),\n",
    "                                   keras.layers.Dense(codings_size)])\n",
    "decoder = keras.models.Sequential([keras.layers.Flatten(input_shape = [codings_size]),\n",
    "                                   keras.layers.Dense(5, activation = 'relu'),\n",
    "                                   keras.layers.Dense(8, activation = 'relu'),\n",
    "                                   keras.layers.Dense(data.shape[1])])\n",
    "ae = keras.models.Sequential([encoder, decoder])\n",
    "ae.compile(optimizer = 'nadam', loss = 'mean_squared_error')\n",
    "ae.fit(data, data, epochs = 100, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained autoencoder we can use it to predict the missing data. Note that i am not taking the whole reconstruction of the encoder for each instance, but instead i am just looking for the output value for the NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on cross validation:  0.609\n"
     ]
    }
   ],
   "source": [
    "reconstructed_data = ae.predict(missing_data)\n",
    "# copying the reconstructed missing values to the original array\n",
    "for zip_ in zips:\n",
    "    missing_data[zip_[0], zip_[1]] = reconstructed_data[zip_[0], zip_[1]]\n",
    "# re-creating the sets\n",
    "data = np.concatenate([data, missing_data])\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "# preprocessing\n",
    "st_sc = StandardScaler()\n",
    "st_sc.fit(X_train)\n",
    "X_train = st_sc.transform(X_train)\n",
    "# cross validating\n",
    "svc = SVC(gamma = 'scale', class_weight = 'balanced')\n",
    "auc = make_scorer(roc_auc_score)\n",
    "scores = cross_val_score(svc, X_train, y_train, cv = 5, scoring = 'accuracy')\n",
    "print('Mean accuracy on cross validation: ', np.round(scores.mean(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so called neural imputing increased the classifier perfomance in a significant way (from 58% to 60.1%). Another interesting comparison is the one with the accuracy of 'simple-imputed' model: the autoencoder outperformed all of them by far. Note that i did not tune neither the autoencoder and the IterativeImputer, so keep this in mind if you want to push perfomance beyond. The last point is about future study: in order to better evaluate the new model we should have to test it with more missing values (maybe realistic ones) and on different datasets. If the model will keep these promising results, we can procced to implement a class and a related library for large scale utilization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
